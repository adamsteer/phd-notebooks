{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on ice thickness estimation from airborne LiDAR\n",
    "Adam Steer, January 2016.\n",
    "adam.d.steer@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we have a bunch of LIDAR points georeferenced relative to an ITRF08 ellipsoid. What we need is the height of each point relative to sea level.\n",
    "\n",
    "But where exactly is the sea surface? We can estimate it from a gravity model, plus a tide model, plus some knowledge of dynamic sea surface topography (wind pressure etc. ).\n",
    "\n",
    "Or we can use a sensor-based approach, since we know some points are returns from water, or extremely thin new ice.\n",
    "\n",
    "This document describes the second approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setup\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.signal import argrelextrema\n",
    "from sklearn import neighbors as nb\n",
    "from shapely import geometry\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setting up a file path for IO\n",
    "infile_dir = '/media/adam/data/is6_f11/lidar.matlab/swaths/'\n",
    "infile = 'is6_f11_pass1_aa_nr2_522816_523019_c.xyz'\n",
    "outfilename = 'is6_f11_pass1_aa_nr2_522816_523019_zi_data.xyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#these parameters drive the sea ice thickness estimation model\n",
    "d_snow = 305\n",
    "sd_dsnow = 10\n",
    "d_ice = 850\n",
    "sd_dice = 10\n",
    "d_water = 1028\n",
    "sd_dwater = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this function is the snow depth model. Parameters come from Steer et al (2016):\n",
    "#Estimating snow depth from altimery for East Antarctic pack ice\n",
    "\n",
    "def compute_zs(tf, s_i, tf_uncert):\n",
    "    \"\"\"\n",
    "    Take in the total freeboard (tf, float array), slope and intercept for an empirical model\n",
    "    of snow depth from elevation (s_i, tuple) and the total freeboard uncertainty\n",
    "    (tf_uncert, float array)\n",
    "\n",
    "    Return a snow depth, with an associated uncertainty.\n",
    "    zs, uncert = compute_zs(tf, ([slope, intercept]), tf_uncert)\n",
    "    \"\"\"\n",
    "    \n",
    "    zs = (params[0] * tf) + s_i[1]\n",
    "\n",
    "    zs_uncert = 0.72 * tf_uncert\n",
    "\n",
    "    return zs, zs_uncert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# next define the model for estimating ice thickness from total freeboard, snow depth\n",
    "# and some density parameters:\n",
    "\n",
    "def compute_zi(tf, zs, d_ice, d_water, d_snow, sd_tf, sd_zs, sd_dsnow, \\\n",
    "               sd_dice, sd_dwater):\n",
    "    \"\"\"\"\n",
    "    sea ice thickness from elevation, and propagation of uncertainties\n",
    "    after Kwok, 2010; kwok and cunningham, 2008\n",
    "    equations:\n",
    "    4: sea ice thickness from elevation\n",
    "    6: taylor series expansion of variance/covariance propagation using\n",
    "    partial derivatives\n",
    "    8, 9 and 10: partial derivatives used in the taylor series\n",
    "    \"\"\"\n",
    "    zi = (d_water / (d_water-d_ice)) * tf - ((d_water-d_snow) / \\\n",
    "         (d_water - d_ice)) * zs\n",
    "\n",
    "    zi_uncert = sd_tf**2 * (d_water / (d_water - d_ice))**2 + \\\n",
    "           sd_zs**2 * ((d_snow - d_water) / (d_water - d_ice))**2 + \\\n",
    "           sd_dsnow**2 * (zs / (d_water - d_ice))**2 + \\\n",
    "           sd_dice**2 * (tf /  (d_water - d_ice))**2 + \\\n",
    "           sd_dwater**2 * (((-d_ice * tf) + ((d_ice-d_snow) * zs)) / \\\n",
    "           (d_water - d_ice)**2)**2\n",
    "\n",
    "    return zi, zi_uncert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# next a utility function to make zero mean data\n",
    "def zero_mean(data):\n",
    "    \"\"\"\n",
    "    take in any array of data\n",
    "    retuun an array modified such that mean(data) == 0\n",
    "    \"\"\"\n",
    "\n",
    "    oldmean = np.mean(data)\n",
    "    return data - oldmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# next, the first function for finding sea level keypoints\n",
    "def find_lowpoints(elev_data, nhood):\n",
    "    \"\"\"\n",
    "    Pass in an array of elevation vlues and an integer  number of points to use\n",
    "    as a neighbourhood.\n",
    "    \n",
    "    Get back the indices of local miminum elevation values for the chosen\n",
    "    neighbourhoods as a tuple.\n",
    "    \n",
    "    http://docs.scipy.org/doc/scipy-0.16.1/reference/generated/\n",
    "    scipy.signal.argrelextrema.html#scipy.signal.argrelextrema\n",
    "    \"\"\"\n",
    "    le_idx = argrelextrema(np.array(elev_data), np.less, order=nhood)\n",
    "    return list(le_idx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#part two of fiding water - intensity percentile based low pass filter\n",
    "def find_low_intens(intens,pntile):\n",
    "    \"\"\"\n",
    "    Take in some return intensity data (array) and a percentile (float)\n",
    "    Return the indices of chosen percentil of intensity values\n",
    "    \"\"\"\n",
    "    \n",
    "    li_idx =  np.where(intens <= np.percentile(intens, pntile))\n",
    "    return list(li_idx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gluing it together, using the three functions above\n",
    "\n",
    "def find_water_keys(xyzi, e_p, i_p):\n",
    "    \"\"\"\n",
    "    Pass in:\n",
    "    - a set of X,Y,Z and Intensity points\n",
    "    - the number of points to use as a neighbourhood for choosing low elevation\n",
    "    - a percentile level for intensity thresholding\n",
    "    Get back:\n",
    "    - a set of XYZI points corresponding to 'local sea level'\n",
    "    \"\"\"\n",
    "    #find lowest intensity values\n",
    "    low_intens_inds = find_low_intens(xyzi[:, 3], i_p)\n",
    "    li_xyzi = xyzi[low_intens_inds, :]\n",
    "\n",
    "    #find low elevations\n",
    "    low_elev_inds = find_lowpoints(li_xyzi[:, 2], e_p)\n",
    "    low_xyzi = li_xyzi[low_elev_inds, :]\n",
    "    #low_xyzi = np.squeeze(low_xyzi)\n",
    "\n",
    "    return low_xyzi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# moving on to some filtering! Point cloud smoothing with respect to 3D space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_tree(pointcloud, tree, radius):\n",
    "    nhoods = tree.query_radius(pointcloud[:,0:3], r=radius)\n",
    "    #new_z = np.median(xyzi[nhoods[0],2])\n",
    "\n",
    "    new_z = []\n",
    "    i = 0\n",
    "    for nhood in nhoods:\n",
    "        #print(nhood)\n",
    "        new_z.append(np.nanmean(pointcloud[nhood[:],2]))\n",
    "\n",
    "        i += 1\n",
    "    return new_z\n",
    "\n",
    "def spatialmedian(pointcloud,radius):\n",
    "    \"\"\"\n",
    "    Using a KDTree and scikit-learn's query_radius method,\n",
    "    ingest an n-d point cloud with the XYZ coordinates occupying\n",
    "    the first three columns respectively (pointcloud[:,0:3]) and a radius in\n",
    "    metres (or whatever units the pointcloud uses).\n",
    "    Returns a new set of Z values which contain the median Z value of points within\n",
    "    radius r of each point.\n",
    "    \"\"\"\n",
    "\n",
    "    from sklearn import neighbors as nb\n",
    "\n",
    "    from datetime import datetime\n",
    "    startTime = datetime.now()\n",
    "\n",
    "    print(startTime)\n",
    "\n",
    "    tree = nb.KDTree(pointcloud[:,0:3], leaf_size=60)\n",
    "\n",
    "    print(datetime.now() - startTime)\n",
    "\n",
    "    nhoods = tree.query_radius(pointcloud[:,0:3], r=radius)\n",
    "    #new_z = np.median(xyzi[nhoods[0],2])\n",
    "\n",
    "    new_z = []\n",
    "    i = 0\n",
    "    for nhood in nhoods:\n",
    "        #print(nhood)\n",
    "        new_z.append(np.median(pointcloud[nhood[:],2]))\n",
    "        #print(pointcloud[i,:])\n",
    "        #print(new_z[i])\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    print(datetime.now() - startTime)\n",
    "\n",
    "    # we really want to return the tree - only generate it once per operation!\n",
    "    return new_z, tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying it all! Setting up reference points and levelling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load up a point cloud\n",
    "lidar = np.genfromtxt(infile_dir + infile)\n",
    "\n",
    "input_points = lidar[(lidar[:,0] >= -150) & (lidar[:,0] <= 130) & \n",
    "                     (lidar[:,1] >= -30) & (lidar[:,1] <= 275) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#pick out XYZ and intensity\n",
    "xyzi = input_points[:, 1:5]\n",
    "\n",
    "#find the lowest elevation point\n",
    "lowestpoint = np.min(input_points[:, 3])\n",
    "\n",
    "#find the sd of intensity\n",
    "sd_intens = np.std(input_points[:, 4])\n",
    "\n",
    "#compute a reference point set\n",
    "xyz_fitpoints = np.squeeze(find_water_keys(xyzi, 100, 1.2))\n",
    "\n",
    "#using a KNeighbours regression, fit a surface to the reference points\n",
    "# just found. Here we declare a KNR object\n",
    "knnf = nb.KNeighborsRegressor(10, algorithm='kd_tree', n_jobs=-1)\n",
    "\n",
    "# here's the fitting part\n",
    "knnf.fit(np.array([xyz_fitpoints[:, 0], xyz_fitpoints[:, 1]]).reshape(len(xyz_fitpoints[:, 1]), 2), xyz_fitpoints[:, 2])\n",
    "\n",
    "#now - predict Z for the entire pointcloud\n",
    "z_fit = knnf.predict(np.array([xyzi[:,0], xyzi[:, 1]]).reshape(len(xyzi[:, 0]), 2))\n",
    "\n",
    "#now, use z_fit to 'level' the data\n",
    "xyzi_mod = np.column_stack([input_points[:,1], input_points[:,2], \\\n",
    "                            input_points[:,3] - z_fit, input_points[:,4]])\n",
    "\n",
    "#next, apply a spatial filter. It is actually the mean of each neighborhood at present\n",
    "median_z, points_kdtree = spatialmedian(xyzi_mod, 2)\n",
    "\n",
    "# OK, now we build a new cloud with median Z\n",
    "xyzi2 = np.column_stack([input_points[:,1], input_points[:,2], \\\n",
    "                            median_z, input_points[:,4]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: making sea ice thickness!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#still unsure why it isn't an no.array already...\n",
    "# assigning the median filtered elevations as total freeboard (tf)\n",
    "tf = np.array(median_z)\n",
    "\n",
    "#this is cheating, but we don't want positive draft!\n",
    "# I'll track these points... \n",
    "tf[np.where(tf < 0)] = 0\n",
    "\n",
    "#get total freeboard uncertainty\n",
    "sd_tf = input_points[:, 8]\n",
    "\n",
    "#compute snow depth based on an empirical model\n",
    "s_i = ([0.72, 0.001])\n",
    "zs, zs_uncert = compute_zs(tf, s_i, sd_tf)\n",
    "\n",
    "#...and finally ice thickness!\n",
    "zi, zi_uncert = compute_zi(tf, zs, d_ice, d_water, d_snow, sd_tf, \\\n",
    "                           zs_uncert, sd_dsnow, sd_dice, sd_dwater)\n",
    "\n",
    "np.mean(zs)\n",
    "np.std(zs)\n",
    "\n",
    "np.mean(zi)\n",
    "np.median(zi)\n",
    "np.std(zi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## To do:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- make a pointcloud class\n",
    "- turn the functions above into methods and functions on the class\n",
    "- for example (a few methods below):\n",
    "    \n",
    "    - pc = pointcloud()\n",
    "    - pc.medianfilter(args)\n",
    "    - pc.tolas(args)\n",
    "    - pc.tonetcdf(args)\n",
    "    - pc.anglefilter(args)\n",
    "    - pc.rangegate(args)\n",
    "    - pc.density(args)\n",
    "    - pc.grid(args)\n",
    "    - pc.normals()\n",
    "    - pc.surf() (maybe)\n",
    "    - pc.outliers()\n",
    "    \n",
    "- and some sea ice specific ones:\n",
    "    - pc.estimatezs(modelparams)\n",
    "    - pc.estimatezi(modelparams)\n",
    "    \n",
    "- and really later:\n",
    "    - pc.make(range, angle, trajectory, transforms)\n",
    "    - pc.computeUncert(args)\n",
    "\n",
    "All these functions exist, and can be called in various ways - but wrapping them up in a class would be nice and neat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
